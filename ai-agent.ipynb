{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gKxRe4tTBHX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_ENDPOINT\")\n",
    "AZURE_OPENAI_VERSION = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "print(AZURE_OPENAI_DEPLOYMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlVbgX39Rhxj"
   },
   "source": [
    "## Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPcrbQfdRrda"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from langchain_core.tools import tool, Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "@tool(\"random_number\")\n",
    "def random_number_maker(input: str):\n",
    "    \"\"\"Returns a random number between 0-100.\"\"\"\n",
    "    return random.randint(0, 100)\n",
    "\n",
    "@tool(\"calculate\")\n",
    "def calculate(num1: int, num2: int, operator: str):\n",
    "    \"\"\"Returns the result of the calculation.\"\"\"\n",
    "    if operator == '+':\n",
    "        result = num1 + num2\n",
    "    elif operator == '-':\n",
    "        result = num1 - num2\n",
    "    elif operator == '*':\n",
    "        result = num1 * num2\n",
    "    elif operator == '/':\n",
    "        result = num1 / num2\n",
    "    elif operator == '^':\n",
    "        result = num1 ** num2\n",
    "    elif operator == '%':\n",
    "        result = num1 % num2\n",
    "    return result\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"\"\"A Python shell. Use this to execute python commands. Input should be a valid python command.\n",
    "                If you want to see the output of a value, you should print it out with `print(...)`.\"\"\",\n",
    "    func=python_repl.run\n",
    ")\n",
    "\n",
    "tavily_search.invoke(\"What is Python?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8OsQ3tIS-_t"
   },
   "source": [
    "## Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5xMQ8ajTxFQ",
    "outputId": "e7bf3e34-f751-42c8-c896-fc37f39a8e3c"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.agents import create_openai_functions_agent, create_json_chat_agent, create_react_agent, AgentExecutor\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=AZURE_OPENAI_VERSION,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "## agent_scratchpad: a function that formats the intermediate steps of the agent's actions and observations into a string\n",
    "# prompt = hub.pull(\"hwchase17/react-chat-json\")\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "tools = [random_number_maker, calculate, tavily_search, repl_tool]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "agent_runnable = create_openai_functions_agent(llm, tools, prompt)\n",
    "# agent_runnable = create_json_chat_agent(llm, tools, prompt)\n",
    "# agent_runnable = create_react_agent(llm, tools, prompt)\n",
    "# agent_executor = AgentExecutor(agent=agent_runnable, tools=tools, verbose=True, return_intermediate_steps=True)\n",
    "\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# history_aware_retriever = create_history_aware_retriever(\n",
    "#     llm, retriever, contextualize_q_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input\": \"List the official 7 wonders of the world.\",\n",
    "    \"chat_history\": [],\n",
    "    \"intermediate_steps\": []\n",
    "}\n",
    "response = agent_runnable.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.bind_tools(tools)\n",
    "\n",
    "model.invoke(\"What is Machine Learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the GraphState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7Fj9KNjQvUq"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    agent_out: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "print(\"GraphState complete..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6pAxC9kcY1F"
   },
   "source": [
    "## Define Nodes for Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JGJGvDVcbvq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def run_agent(state: list):\n",
    "    print(\"> run_agent\")\n",
    "    agent_out = agent_runnable.invoke(state)\n",
    "    return {\"agent_out\": agent_out}\n",
    "\n",
    "def execute_tools(state: list):\n",
    "    print(\"> execute_tools\")\n",
    "    agent_action = state[\"agent_out\"]\n",
    "    output = tool_executor.invoke(agent_action)\n",
    "    print(f\"The agent action is: {agent_action}\")\n",
    "    print(f\"The tool result is: {output}\")\n",
    "    return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
    "\n",
    "def router(state: list):\n",
    "    print(\"> router\")\n",
    "    if isinstance(state['agent_out'], AgentFinish):\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def rag_final_answer(state: list):\n",
    "    print(\"> rag_final_answer\")\n",
    "    query = state[\"input\"]\n",
    "    context = state[\"intermediate_steps\"]\n",
    "    print(f\"Input: {query}\")\n",
    "    print(f\"Context: {context}\")\n",
    "\n",
    "    prompt = f\"\"\"You are a great researcher who conducts very technical research around a given\n",
    "    query and context. Your research outcome only contains facts and urls to support those facts\n",
    "    in that order.\n",
    "    CONTEXT: {context}\n",
    "    QUESTION: {query}\"\"\"\n",
    "    out = model.invoke(prompt)\n",
    "    return {\"agent_out\": out}\n",
    "\n",
    "print(\"Node definitions complete..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwv4EsTKeZzh"
   },
   "source": [
    "## Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAkcBE5pebXv"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", run_agent)\n",
    "graph.add_node(\"tools\", execute_tools)\n",
    "graph.add_node(\"final_answer\", rag_final_answer)\n",
    "\n",
    "# conditional edges are controlled by router\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": \"final_answer\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "graph.add_edge(\"final_answer\", END)\n",
    "\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "graph_runnable = graph.compile()\n",
    "\n",
    "print(\"Graph complete..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "EmwkoQkthb__",
    "outputId": "e7ec1501-557f-4ec6-f604-8764dc90a78e"
   },
   "outputs": [],
   "source": [
    "graph.branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.nodes, graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DuDCW775wkC",
    "outputId": "4996eddf-1c0e-4bf1-addf-8522328f6a4a"
   },
   "outputs": [],
   "source": [
    "graph_inputs = {\n",
    "    \"input\": \"What are the results for the 4 fundamental calculations of 2 random numbers?\",\n",
    "    \"chat_history\": [],\n",
    "    \"intermediate_steps\": []\n",
    "}\n",
    "graph_runnable.invoke(graph_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = graph_runnable.invoke({\n",
    "    \"input\": \"Please provide a summary regarding Canadian immigration policies.\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(out[\"agent_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = graph_runnable.invoke({\n",
    "    \"input\": \"What was the ice hockey finals series and game score?\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = graph_runnable.invoke({\n",
    "    \"input\": \"What is the date tomorrow and the weather in Toronto then?\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(out[\"agent_out\"])\n",
    "# print(out.strip('content='))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_runnable.invoke({\n",
    "    \"input\": \"Create a bar graph resembling a gaussian distribution and find the probability density function formula.\",\n",
    "    \"chat_history\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reWvgJxJfjlo",
    "outputId": "7d2e221f-b88c-47bc-cd11-a91ae4f98f58"
   },
   "outputs": [],
   "source": [
    "out = graph_runnable.invoke({\n",
    "    \"input\": \"hi\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Xhdimwmgezx",
    "outputId": "8c109869-7f01-437c-cc5f-bb2d4bf24826"
   },
   "outputs": [],
   "source": [
    "out = graph_runnable.invoke({\n",
    "    \"input\": \"hi, please don't respond to me with a `source`\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(out[\"agent_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
